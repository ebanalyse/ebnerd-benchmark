{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello\n",
    "\n",
    "This notebook is an example of how to make a beyond-accuracy dataset, and how one could make baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import *\n",
    "from ebrec.evaluation.beyond_accuracy import (\n",
    "    IntralistDiversity,\n",
    "    Distribution,\n",
    "    Serendipity,\n",
    "    Sentiment,\n",
    "    Coverage,\n",
    "    Novelty,\n",
    ")\n",
    "from ebrec.utils._articles import create_sort_based_prediction_score\n",
    "from ebrec.utils._behaviors import truncate_history\n",
    "from ebrec.utils._polars import slice_join_dataframes\n",
    "from ebrec.utils._python import (\n",
    "    rank_predictions_by_score,\n",
    "    write_submission_file,\n",
    "    write_json_file,\n",
    "    read_json_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASPLIT = \"ebnerd_large\"\n",
    "# ROOT PATH:\n",
    "PATH = Path(\"~/ebnerd_data\").expanduser()\n",
    "#\n",
    "## We are using the LARGE articles dataset to ensure we have all articles IDs,\n",
    "# for beyond-accuracy; as only 154 aids are found in the demo.\n",
    "ARTICLES_PATH = PATH.joinpath(\"articles.parquet\")\n",
    "\n",
    "\n",
    "# PATH TO DUMP ARTIFACTS:\n",
    "PATH_BEYOND_ACCURACY = Path(\"ebnerd_predictions\")\n",
    "PATH_BEYOND_ACCURACY.mkdir(exist_ok=True, parents=True)\n",
    "# BASELINE ARTIFACTS:\n",
    "PATH_BEYOND_ACCURACY_BASELINES = PATH_BEYOND_ACCURACY.joinpath(\"baselines\")\n",
    "PATH_BEYOND_ACCURACY_BASELINES.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEYOND_ACCURACY_HISTORY_DICT = \"beyond_accuracy_history_dict.json\"\n",
    "BEYOND_ACCURACY_USERS_DICT = \"beyond_accuracy_users_dict.json\"\n",
    "CANDIDATE_LIST = \"candidate_list.json\"\n",
    "ARTICLES_DICT = \"articles_dict.json\"\n",
    "BEHAVIORS_TIMESTAMP_DICT = \"behaviors_timestamp_dict.json\"\n",
    "#\n",
    "BASELINE_DIVERSITY = \"intralist_diversity.json\"\n",
    "BASELINE_SENTIMENT_SCORE = \"sentiment_score.json\"\n",
    "BASELINE_NOVELTY = \"novelty.json\"\n",
    "BASELINE_SERENDIPITY = \"serendipity.json\"\n",
    "BASELINE_COVERAGE = \"coverage.json\"\n",
    "BASELINE_DISTRIBUTION_CATEGORY = \"distribution_category.json\"\n",
    "BASELINE_DISTRIBUTION_SENTIMENT_LABEL = \"distribution_sentiment_label.json\"\n",
    "BASELINE_DISTRIBUTION_TOPICS = \"distribution_topics.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "df_beyond_accuarcy = pl.scan_parquet(\n",
    "    PATH.joinpath(\"ebnerd_testset\", \"test\", \"behaviors.parquet\")\n",
    ").filter(pl.col(\"is_beyond_accuracy\"))\n",
    "df_behaviors = pl.scan_parquet(\n",
    "    PATH.joinpath(\"ebnerd_testset\", \"test\", \"behaviors.parquet\")\n",
    ").filter(~pl.col(\"is_beyond_accuracy\"))\n",
    "df_articles = pl.scan_parquet(ARTICLES_PATH)\n",
    "df_history = pl.scan_parquet(\n",
    "    PATH.joinpath(\"ebnerd_testset\", \"test\", \"history.parquet\")\n",
    ").select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make / Dump Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the candidate list from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/candidate_list.json'\n",
      "Number of Candidate IDs: 250 (example: [9793163, 9793069, 9792076, 9792749, 9791280])\n"
     ]
    }
   ],
   "source": [
    "candidate_list = (\n",
    "    df_beyond_accuarcy.select(pl.col(DEFAULT_INVIEW_ARTICLES_COL).first())\n",
    "    .collect()\n",
    "    .to_series()\n",
    ")[0].to_list()\n",
    "\n",
    "\n",
    "write_json_file(\n",
    "    candidate_list, PATH_BEYOND_ACCURACY.joinpath(CANDIDATE_LIST), verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Number of Candidate IDs: {len(candidate_list)} (example: {candidate_list[:5]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "santity check - passed\n"
     ]
    }
   ],
   "source": [
    "load_candidate_list = read_json_file(PATH_BEYOND_ACCURACY.joinpath(CANDIDATE_LIST))\n",
    "if (\n",
    "    not (\n",
    "        df_beyond_accuarcy.select(DEFAULT_INVIEW_ARTICLES_COL).collect()\n",
    "        == candidate_list\n",
    "    )\n",
    "    .sum()[DEFAULT_INVIEW_ARTICLES_COL]\n",
    "    .to_list()[0]\n",
    "    == df_beyond_accuarcy.select(DEFAULT_INVIEW_ARTICLES_COL).collect().shape[0]\n",
    "):\n",
    "    raise ValueError(\"candidate_list is not identical in the testset\")\n",
    "\n",
    "if not (np.array(candidate_list) - np.array(load_candidate_list)).sum() == 0:\n",
    "    raise ValueError(\"candidate_list was not dump correctly\")\n",
    "\n",
    "print(\"santity check - passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User meta data: Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/beyond_accuracy_users_dict.json'\n",
      "#rows: 200000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>is_subscriber</th><th>is_sso_user</th><th>postcode</th><th>gender</th><th>age</th></tr><tr><td>bool</td><td>bool</td><td>i8</td><td>i8</td><td>i8</td></tr></thead><tbody><tr><td>true</td><td>true</td><td>null</td><td>0</td><td>null</td></tr><tr><td>true</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>true</td><td>true</td><td>null</td><td>0</td><td>50</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 5)\n",
       "┌───────────────┬─────────────┬──────────┬────────┬──────┐\n",
       "│ is_subscriber ┆ is_sso_user ┆ postcode ┆ gender ┆ age  │\n",
       "│ ---           ┆ ---         ┆ ---      ┆ ---    ┆ ---  │\n",
       "│ bool          ┆ bool        ┆ i8       ┆ i8     ┆ i8   │\n",
       "╞═══════════════╪═════════════╪══════════╪════════╪══════╡\n",
       "│ true          ┆ true        ┆ null     ┆ 0      ┆ null │\n",
       "│ true          ┆ true        ┆ null     ┆ null   ┆ null │\n",
       "│ true          ┆ true        ┆ null     ┆ 0      ┆ 50   │\n",
       "└───────────────┴─────────────┴──────────┴────────┴──────┘"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_meta_columns = [\n",
    "    DEFAULT_IS_SUBSCRIBER_COL,\n",
    "    DEFAULT_IS_SSO_USER_COL,\n",
    "    DEFAULT_POSTCODE_COL,\n",
    "    DEFAULT_GENDER_COL,\n",
    "    DEFAULT_AGE_COL,\n",
    "]\n",
    "df_users = df_beyond_accuarcy.select(pl.col(user_meta_columns)).collect()\n",
    "\n",
    "users_dict = {col: df_users[col].to_list() for col in df_users.columns}\n",
    "write_json_file(\n",
    "    users_dict, PATH_BEYOND_ACCURACY.joinpath(BEYOND_ACCURACY_USERS_DICT), verbose=True\n",
    ")\n",
    "print(f\"#rows: {df_users.shape[0]}\")\n",
    "df_users.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/beyond_accuracy_history_dict.json'\n",
      "#rows: 200000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th></tr><tr><td>u32</td><td>list[i32]</td></tr></thead><tbody><tr><td>1049297</td><td>[9788862, 9788067, … 9787586]</td></tr><tr><td>231624</td><td>[9789910, 9789704, … 9790811]</td></tr><tr><td>716356</td><td>[9785062, 9772508, … 9786313]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "┌─────────┬───────────────────────────────┐\n",
       "│ user_id ┆ article_id_fixed              │\n",
       "│ ---     ┆ ---                           │\n",
       "│ u32     ┆ list[i32]                     │\n",
       "╞═════════╪═══════════════════════════════╡\n",
       "│ 1049297 ┆ [9788862, 9788067, … 9787586] │\n",
       "│ 231624  ┆ [9789910, 9789704, … 9790811] │\n",
       "│ 716356  ┆ [9785062, 9772508, … 9786313] │\n",
       "└─────────┴───────────────────────────────┘"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HISTORY_SIZE = 20\n",
    "df_history_truncate = df_history.pipe(\n",
    "    truncate_history,\n",
    "    column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    history_size=HISTORY_SIZE,\n",
    "    padding_value=None,\n",
    "    enable_warning=False,\n",
    ")\n",
    "# =>\n",
    "df_user_histoies = (\n",
    "    df_beyond_accuarcy.select(DEFAULT_USER_COL)\n",
    "    .join(df_history_truncate, on=DEFAULT_USER_COL, how=\"left\")\n",
    "    .collect()\n",
    ")\n",
    "user_history_dict = {\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL: df_user_histoies[\n",
    "        DEFAULT_HISTORY_ARTICLE_ID_COL\n",
    "    ].to_list()\n",
    "}\n",
    "write_json_file(\n",
    "    user_history_dict,\n",
    "    PATH_BEYOND_ACCURACY.joinpath(BEYOND_ACCURACY_HISTORY_DICT),\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"#rows: {df_user_histoies.shape[0]}\")\n",
    "df_user_histoies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp for Behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for computing the AUC as function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/behaviors_timestamp_dict.json'\n",
      "#rows: 13336710\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>impression_time</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;2023-06-05 15:…</td></tr><tr><td>&quot;2023-06-05 15:…</td></tr><tr><td>&quot;2023-06-05 15:…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 1)\n",
       "┌────────────────────────────┐\n",
       "│ impression_time            │\n",
       "│ ---                        │\n",
       "│ str                        │\n",
       "╞════════════════════════════╡\n",
       "│ 2023-06-05 15:02:49.000000 │\n",
       "│ 2023-06-05 15:03:56.000000 │\n",
       "│ 2023-06-05 15:25:53.000000 │\n",
       "└────────────────────────────┘"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_behaviors_timestamp = df_behaviors.select(\n",
    "    pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).cast(pl.Utf8),\n",
    ").collect()\n",
    "behaviors_timestamp_dict = {\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL: df_behaviors_timestamp[\n",
    "        DEFAULT_IMPRESSION_TIMESTAMP_COL\n",
    "    ].to_list()\n",
    "}\n",
    "write_json_file(\n",
    "    behaviors_timestamp_dict,\n",
    "    PATH_BEYOND_ACCURACY.joinpath(BEHAVIORS_TIMESTAMP_DICT),\n",
    "    verbose=True,\n",
    ")\n",
    "print(f\"#rows: {df_behaviors_timestamp.shape[0]}\")\n",
    "df_behaviors_timestamp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Candidate lookup dict / Dump lookup dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles to include: *candidate-list* and *history-articles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#history_article_id: 14228)\n"
     ]
    }
   ],
   "source": [
    "history_article_id = (\n",
    "    df_user_histoies.lazy()\n",
    "    .select(pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).explode().unique())\n",
    "    .collect()[DEFAULT_HISTORY_ARTICLE_ID_COL]\n",
    "    .to_list()\n",
    ")\n",
    "print(f\"#history_article_id: {len(history_article_id)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the different datasizes (*demo*, *small*, and *large*) has subset of the total article-catelog. Hence, if you're using *demo*, not all of the articles in the candidate-list may be in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#articles: 14478 (#candidate_list: 250 & #history_article_id: 14228)\n"
     ]
    }
   ],
   "source": [
    "aids_in_split = (\n",
    "    df_articles.select(DEFAULT_ARTICLE_ID_COL)\n",
    "    .collect()[DEFAULT_ARTICLE_ID_COL]\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "history_article_id = [id for id in history_article_id if id in aids_in_split]\n",
    "candidate_list = [id for id in candidate_list if id in aids_in_split]\n",
    "\n",
    "article_ids = candidate_list + history_article_id\n",
    "print(\n",
    "    f\"#articles: {len(article_ids)} (#candidate_list: {len(candidate_list)} & #history_article_id: {len(history_article_id)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select articles that should be included in the lookup dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_lookup_articles shape: (14478, 21)\n"
     ]
    }
   ],
   "source": [
    "# =>\n",
    "df_lookup_articles = (\n",
    "    df_articles.filter(pl.col(DEFAULT_ARTICLE_ID_COL).is_in(article_ids))\n",
    "    .with_columns(\n",
    "        pl.col(\n",
    "            DEFAULT_ARTICLE_MODIFIED_TIMESTAMP_COL,\n",
    "            DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL,\n",
    "        ).cast(pl.Utf8)\n",
    "    )\n",
    "    # Zeros might cause issues\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_TOTAL_INVIEWS_COL, DEFAULT_TOTAL_PAGEVIEWS_COL).fill_null(1)\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "print(f\"df_lookup_articles shape: {df_lookup_articles.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make normalize popularity-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>total_pageviews_normalized_max</th><th>total_pageviews_normalized_min_max</th></tr><tr><td>str</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>14478.0</td><td>14478.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>0.018543</td><td>0.018641</td></tr><tr><td>&quot;std&quot;</td><td>0.034784</td><td>0.034781</td></tr><tr><td>&quot;min&quot;</td><td>6.1059e-7</td><td>0.0001</td></tr><tr><td>&quot;25%&quot;</td><td>6.1059e-7</td><td>0.0001</td></tr><tr><td>&quot;50%&quot;</td><td>6.1059e-7</td><td>0.0001</td></tr><tr><td>&quot;75%&quot;</td><td>0.028582</td><td>0.028679</td></tr><tr><td>&quot;max&quot;</td><td>1.0</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 3)\n",
       "┌────────────┬────────────────────────────────┬───────────────────────────────────┐\n",
       "│ statistic  ┆ total_pageviews_normalized_max ┆ total_pageviews_normalized_min_m… │\n",
       "│ ---        ┆ ---                            ┆ ---                               │\n",
       "│ str        ┆ f64                            ┆ f64                               │\n",
       "╞════════════╪════════════════════════════════╪═══════════════════════════════════╡\n",
       "│ count      ┆ 14478.0                        ┆ 14478.0                           │\n",
       "│ null_count ┆ 0.0                            ┆ 0.0                               │\n",
       "│ mean       ┆ 0.018543                       ┆ 0.018641                          │\n",
       "│ std        ┆ 0.034784                       ┆ 0.034781                          │\n",
       "│ min        ┆ 6.1059e-7                      ┆ 0.0001                            │\n",
       "│ 25%        ┆ 6.1059e-7                      ┆ 0.0001                            │\n",
       "│ 50%        ┆ 6.1059e-7                      ┆ 0.0001                            │\n",
       "│ 75%        ┆ 0.028582                       ┆ 0.028679                          │\n",
       "│ max        ┆ 1.0                            ┆ 1.0                               │\n",
       "└────────────┴────────────────────────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_TOTAL_PAGEVIEWS_COL_NORMALIZED_MAX = (\n",
    "    DEFAULT_TOTAL_PAGEVIEWS_COL + \"_normalized_max\"\n",
    ")\n",
    "DEFAULT_TOTAL_PAGEVIEWS_COL_NORMALIZED_MIN_MAX = (\n",
    "    DEFAULT_TOTAL_PAGEVIEWS_COL + \"_normalized_min_max\"\n",
    ")\n",
    "\n",
    "MIN_X = df_lookup_articles[DEFAULT_TOTAL_PAGEVIEWS_COL].min()\n",
    "MAX_X = df_lookup_articles[DEFAULT_TOTAL_PAGEVIEWS_COL].max()\n",
    "MIN_RANGE = 1e-4\n",
    "MAX_RANGE = 1.0\n",
    "\n",
    "df_lookup_articles = df_lookup_articles.with_columns(\n",
    "    (  # SIMPLE MAX NORMALIZATION: x / max()\n",
    "        pl.col(DEFAULT_TOTAL_PAGEVIEWS_COL) / pl.col(DEFAULT_TOTAL_PAGEVIEWS_COL).max()\n",
    "    ).alias(DEFAULT_TOTAL_PAGEVIEWS_COL_NORMALIZED_MAX)\n",
    ").with_columns(\n",
    "    (  #  MIN-MAX NORMALIZATION: ( x_i − X_min ⁡ ) / ( X_max ⁡ − X_min ⁡ ) * (max_range − min_range) + min_range\n",
    "        ((pl.col(DEFAULT_TOTAL_PAGEVIEWS_COL) - MIN_X) / (MAX_X - MIN_X))\n",
    "        * (MAX_RANGE - MIN_RANGE)\n",
    "        + MIN_RANGE\n",
    "    ).alias(\n",
    "        DEFAULT_TOTAL_PAGEVIEWS_COL_NORMALIZED_MIN_MAX\n",
    "    )\n",
    ")\n",
    "\n",
    "df_lookup_articles.select(\n",
    "    DEFAULT_TOTAL_PAGEVIEWS_COL_NORMALIZED_MAX,\n",
    "    DEFAULT_TOTAL_PAGEVIEWS_COL_NORMALIZED_MIN_MAX,\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add embeddings representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#rows: 14478\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 25)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th><th>total_pageviews_normalized_max</th><th>total_pageviews_normalized_min_max</th><th>contrastive_vector</th><th>document_vector</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td><td>f64</td><td>f64</td><td>list[f32]</td><td>list[f32]</td></tr></thead><tbody><tr><td>3005351</td><td>&quot;Sainz bekræfte…</td><td>&quot;FODBOLD: Carlo…</td><td>&quot;2023-06-29 06:…</td><td>false</td><td>&quot;Michael Laudru…</td><td>&quot;2006-05-15 20:…</td><td>[3170015]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Erhverv&quot;, &quot;Kendt&quot;, … &quot;Politik&quot;]</td><td>142</td><td>[196, 227]</td><td>&quot;sport&quot;</td><td>1</td><td>1</td><td>null</td><td>0.8204</td><td>&quot;Neutral&quot;</td><td>6.1059e-7</td><td>0.0001</td><td>[-0.081303, 0.012837, … 0.025402]</td><td>[0.030043, 0.014314, … 0.069532]</td></tr><tr><td>3006206</td><td>&quot;Ny sex-skandal…</td><td>&quot;Stjernen fra &#x27;…</td><td>&quot;2023-06-29 06:…</td><td>false</td><td>&quot;Den engelske s…</td><td>&quot;2007-03-06 08:…</td><td>[3152747]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kendt&quot;, &quot;Livsstil&quot;, … &quot;Kultur&quot;]</td><td>414</td><td>[432]</td><td>&quot;underholdning&quot;</td><td>1</td><td>1</td><td>null</td><td>0.9295</td><td>&quot;Negative&quot;</td><td>6.1059e-7</td><td>0.0001</td><td>[-0.050866, 0.063385, … -0.030806]</td><td>[0.114115, 0.00947, … 0.04868]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 25)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_pag ┆ contrasti ┆ document │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews_no ┆ eviews_no ┆ ve_vector ┆ _vector  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ rmalized_ ┆ rmalized_ ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ str       ┆   ┆ max       ┆ min_m…    ┆ list[f32] ┆ list[f32 │\n",
       "│           ┆           ┆           ┆           ┆   ┆ ---       ┆ ---       ┆           ┆ ]        │\n",
       "│           ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3005351   ┆ Sainz     ┆ FODBOLD:  ┆ 2023-06-2 ┆ … ┆ 6.1059e-7 ┆ 0.0001    ┆ [-0.08130 ┆ [0.03004 │\n",
       "│           ┆ bekræfter ┆ Carlos    ┆ 9 06:20:3 ┆   ┆           ┆           ┆ 3,        ┆ 3, 0.014 │\n",
       "│           ┆ Laudrup-i ┆ Sainz,    ┆ 5.000000  ┆   ┆           ┆           ┆ 0.012837, ┆ 314, …   │\n",
       "│           ┆ nteress…  ┆ vicepræsi ┆           ┆   ┆           ┆           ┆ …         ┆ 0.069532 │\n",
       "│           ┆           ┆ …         ┆           ┆   ┆           ┆           ┆ 0.025402… ┆ ]        │\n",
       "│ 3006206   ┆ Ny sex-sk ┆ Stjernen  ┆ 2023-06-2 ┆ … ┆ 6.1059e-7 ┆ 0.0001    ┆ [-0.05086 ┆ [0.11411 │\n",
       "│           ┆ andale    ┆ fra 'Den  ┆ 9 06:20:3 ┆   ┆           ┆           ┆ 6,        ┆ 5,       │\n",
       "│           ┆ for Ralph ┆ Engelske  ┆ 6.000000  ┆   ┆           ┆           ┆ 0.063385, ┆ 0.00947, │\n",
       "│           ┆ Fienne…   ┆ Patie…    ┆           ┆   ┆           ┆           ┆ …         ┆ …        │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ -0.03080… ┆ 0.04868] │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# => Embeddings:\n",
    "BERT_VECTOR = \"bert_base_multilingual_cased\"\n",
    "ROBERTA_VECTOR = \"xlm_roberta_base\"\n",
    "\n",
    "CONTRASTIVE_VECTOR = \"contrastive_vector\"\n",
    "DOCUMENT_VECTOR = \"document_vector\"\n",
    "\n",
    "\n",
    "def load_join_embeddings(df: pl.DataFrame, emb_path: Path) -> pl.DataFrame:\n",
    "    emb_contrastive = (\n",
    "        pl.scan_parquet(PATH.parent.joinpath(emb_path))\n",
    "        .filter(pl.col(DEFAULT_ARTICLE_ID_COL).is_in(df.select(DEFAULT_ARTICLE_ID_COL)))\n",
    "        .collect()\n",
    "    )\n",
    "    return df.join(emb_contrastive, on=DEFAULT_ARTICLE_ID_COL, how=\"left\")\n",
    "\n",
    "\n",
    "df_lookup_articles = df_lookup_articles.pipe(\n",
    "    load_join_embeddings,\n",
    "    emb_path=PATH.joinpath(\n",
    "        \"artifacts\", \"Ekstra_Bladet_contrastive_vector\", f\"{CONTRASTIVE_VECTOR}.parquet\"\n",
    "    ),\n",
    ").pipe(\n",
    "    load_join_embeddings,\n",
    "    emb_path=PATH.joinpath(\n",
    "        \"artifacts\", \"Ekstra_Bladet_word2vec\", f\"{DOCUMENT_VECTOR}.parquet\"\n",
    "    ),\n",
    ")\n",
    "print(f\"#rows: {df_lookup_articles.shape[0]}\")\n",
    "df_lookup_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to lookup dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/articles_dict.json'\n",
      "#articles: 14478\n"
     ]
    }
   ],
   "source": [
    "articles_dict = {}\n",
    "for row in df_lookup_articles.iter_rows(named=True):\n",
    "    # Note, all keys in dictionaries are converted to strings, when serializing an object to JSON format.\n",
    "    articles_dict[str(row[DEFAULT_ARTICLE_ID_COL])] = row\n",
    "# Write it:\n",
    "write_json_file(\n",
    "    articles_dict, PATH_BEYOND_ACCURACY.joinpath(ARTICLES_DICT), verbose=True\n",
    ")\n",
    "print(f\"#articles: {len(articles_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a couple *Baselines* based on the candidate-list:\n",
    "1. @EditorialPicks: We approximate this based on the number **inview** an articles have recived. Ekstra Bladet is front-page driven, meaning, if an article has a lot of inview-impression (seen) a lot, we believe it has been selected to be in a top priority from the editors. This is static (it does change for our *candidate_list*), i.e., the computation is done once.\n",
    "2. @Popular: We approximate this based on the number **clicks** an articles have recived. This is static (it does change for our *candidate_list*), i.e., the computation is done once.\n",
    "3. @Random: Simple baseline and important baseline. We simple pick a set of *top-n* articles from the *candidate-list* and run multiple times.\n",
    "4. @Dissimilarity / Similarity (will come later): Select top-n articles that are the most similar / dissimilar. \n",
    "5. @Newest: Simply pick the newest released articles. We do see newssite where the top banner is *Newest released*. We include it, but note this is very sensitive and might not be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#behaviors_timestamp_dict: 13336710\n",
      "#history_dict: 200000\n",
      " history_dict.keys(): dict_keys(['article_id_fixed'])\n",
      "#users_dict 200000\n",
      " users_dict.keys(): dict_keys(['is_subscriber', 'is_sso_user', 'postcode', 'gender', 'age'])\n",
      "#user_history_dict 200000\n",
      " users_dict.keys(): dict_keys(['article_id_fixed'])\n",
      "#articles_dict: 14478\n",
      " articles_dict[ID].keys(): dict_keys(['article_id', 'title', 'subtitle', 'last_modified_time', 'premium', 'body', 'published_time', 'image_ids', 'article_type', 'url', 'ner_clusters', 'entity_groups', 'topics', 'category', 'subcategory', 'category_str', 'total_inviews', 'total_pageviews', 'total_read_time', 'sentiment_score', 'sentiment_label', 'total_pageviews_normalized_max', 'total_pageviews_normalized_min_max', 'contrastive_vector', 'document_vector'])\n",
      "#candidate_list: 250\n",
      "#candidate-articles (df): 250\n"
     ]
    }
   ],
   "source": [
    "def n_items(d) -> int:\n",
    "    return len(d[list(d)[0]])\n",
    "\n",
    "\n",
    "# =>\n",
    "behaviors_timestamp_dict = read_json_file(\n",
    "    PATH_BEYOND_ACCURACY.joinpath(BEHAVIORS_TIMESTAMP_DICT)\n",
    ")\n",
    "print(f\"#behaviors_timestamp_dict: {n_items(behaviors_timestamp_dict)}\")\n",
    "\n",
    "# =>\n",
    "history_dict = read_json_file(\n",
    "    PATH_BEYOND_ACCURACY.joinpath(BEYOND_ACCURACY_HISTORY_DICT)\n",
    ")\n",
    "print(\n",
    "    f\"#history_dict: {n_items(history_dict)}\\n history_dict.keys(): {history_dict.keys()}\"\n",
    ")\n",
    "\n",
    "# =>\n",
    "users_dict = read_json_file(PATH_BEYOND_ACCURACY.joinpath(BEYOND_ACCURACY_USERS_DICT))\n",
    "print(f\"#users_dict {n_items(users_dict)}\\n users_dict.keys(): {users_dict.keys()}\")\n",
    "\n",
    "# =>\n",
    "user_history_dict = read_json_file(\n",
    "    PATH_BEYOND_ACCURACY.joinpath(BEYOND_ACCURACY_HISTORY_DICT)\n",
    ")\n",
    "print(\n",
    "    f\"#user_history_dict {n_items(user_history_dict)}\\n users_dict.keys(): {user_history_dict.keys()}\"\n",
    ")\n",
    "\n",
    "# =>\n",
    "articles_dict = {\n",
    "    int(key): val\n",
    "    for key, val in read_json_file(PATH_BEYOND_ACCURACY.joinpath(ARTICLES_DICT)).items()\n",
    "}\n",
    "aid_keys = articles_dict[list(articles_dict)[0]].keys()\n",
    "print(f\"#articles_dict: {len(articles_dict)}\\n articles_dict[ID].keys(): {aid_keys}\")\n",
    "\n",
    "# => Only the once actually found in the dataset (for demo only 154 of 250 are represent)\n",
    "candidate_list = [\n",
    "    id\n",
    "    for id in read_json_file(PATH_BEYOND_ACCURACY.joinpath(CANDIDATE_LIST))\n",
    "    if id in list(articles_dict)\n",
    "]\n",
    "print(f\"#candidate_list: {len(candidate_list)}\")\n",
    "\n",
    "df_candidate_articles = df_lookup_articles.filter(\n",
    "    pl.col(DEFAULT_ARTICLE_ID_COL).is_in(candidate_list)\n",
    ")\n",
    "print(f\"#candidate-articles (df): {df_candidate_articles.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Ranked Candidate lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editorial (Top Inview Articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9790335 9791587]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>total_inviews</th><th>prediction_score</th></tr><tr><td>i32</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>9790335</td><td>1698890</td><td>1.0</td></tr><tr><td>9791587</td><td>1369829</td><td>0.5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 3)\n",
       "┌────────────┬───────────────┬──────────────────┐\n",
       "│ article_id ┆ total_inviews ┆ prediction_score │\n",
       "│ ---        ┆ ---           ┆ ---              │\n",
       "│ i32        ┆ i32           ┆ f64              │\n",
       "╞════════════╪═══════════════╪══════════════════╡\n",
       "│ 9790335    ┆ 1698890       ┆ 1.0              │\n",
       "│ 9791587    ┆ 1369829       ┆ 0.5              │\n",
       "└────────────┴───────────────┴──────────────────┘"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidates_editorial_picks = create_sort_based_prediction_score(\n",
    "    df_candidate_articles,\n",
    "    column=DEFAULT_TOTAL_INVIEWS_COL,\n",
    "    desc=True,\n",
    ")\n",
    "candidates_editorial_picks = np.array(\n",
    "    [df_candidates_editorial_picks.select(DEFAULT_ARTICLE_ID_COL).to_series()]\n",
    ")\n",
    "# =>\n",
    "print(candidates_editorial_picks[:, :2])\n",
    "df_candidates_editorial_picks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9791428 9792719]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>total_pageviews</th><th>prediction_score</th></tr><tr><td>i32</td><td>i32</td><td>f64</td></tr></thead><tbody><tr><td>9791428</td><td>256541</td><td>1.0</td></tr><tr><td>9792719</td><td>209050</td><td>0.5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 3)\n",
       "┌────────────┬─────────────────┬──────────────────┐\n",
       "│ article_id ┆ total_pageviews ┆ prediction_score │\n",
       "│ ---        ┆ ---             ┆ ---              │\n",
       "│ i32        ┆ i32             ┆ f64              │\n",
       "╞════════════╪═════════════════╪══════════════════╡\n",
       "│ 9791428    ┆ 256541          ┆ 1.0              │\n",
       "│ 9792719    ┆ 209050          ┆ 0.5              │\n",
       "└────────────┴─────────────────┴──────────────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_candidates_popular = create_sort_based_prediction_score(\n",
    "    df_candidate_articles,\n",
    "    column=DEFAULT_TOTAL_PAGEVIEWS_COL,\n",
    "    desc=True,\n",
    ")\n",
    "candidates_popular = np.array(\n",
    "    [df_candidates_popular.select(DEFAULT_ARTICLE_ID_COL).to_series()]\n",
    ")\n",
    "# =>\n",
    "print(candidates_popular[:, :2])\n",
    "df_candidates_popular.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "intralist_diversity = IntralistDiversity()\n",
    "distribution = Distribution()\n",
    "serendipity = Serendipity()\n",
    "sentiment = Sentiment()\n",
    "coverage = Coverage()\n",
    "novelty = Novelty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Baselines (and your model)\n",
    "\n",
    "### Random-baseline\n",
    "Note, we're just running a random baseline with 200,000 iterations. This means that, based on the distribution, we would expect segments to be equally represented. However, you need to increase this number before you see them truly balance out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#random-iterations: 200000\n",
      "Top@5 ranked articles\n"
     ]
    }
   ],
   "source": [
    "RANDOM_ITER = df_beyond_accuarcy.select(DEFAULT_INVIEW_ARTICLES_COL).collect().shape[0]\n",
    "TOP_N = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "# Make list:\n",
    "top_n_candidates_random = [\n",
    "    np.random.choice(candidate_list, size=TOP_N, replace=False)\n",
    "    for _ in range(RANDOM_ITER)\n",
    "]\n",
    "top_n_candidates_editorial_picks = candidates_editorial_picks[:, :TOP_N]\n",
    "top_n_candidates_popular = candidates_popular[:, :TOP_N]\n",
    "#\n",
    "# Set them as tuples, just to loop through it:\n",
    "candidates_name_pairs = [\n",
    "    [top_n_candidates_editorial_picks, \"top-inview-articles\"],\n",
    "    [top_n_candidates_popular, \"popular\"],\n",
    "    [top_n_candidates_random, \"random\"],\n",
    "]\n",
    "# =>\n",
    "user_history = user_history_dict[DEFAULT_HISTORY_ARTICLE_ID_COL]\n",
    "\n",
    "print(f\"#random-iterations: {RANDOM_ITER}\")\n",
    "print(f\"Top@{TOP_N} ranked articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Model\n",
    "Try to add your model's prediction of the candidate list. In this notebook we just take a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_your_model = [\n",
    "    np.random.choice(candidate_list, size=TOP_N, replace=False)\n",
    "    for _ in range(RANDOM_ITER)\n",
    "]\n",
    "## Just uncomment and add the prediction and name:\n",
    "# candidates_name_pairs.append([candidates_your_model, \"random_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instralist-Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/intralist_diversity.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>top-inview-articles_intralist_diversity</th><th>popular_intralist_diversity</th><th>random_intralist_diversity</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mean&quot;</td><td>0.790542</td><td>0.840236</td><td>0.754795</td></tr><tr><td>&quot;std&quot;</td><td>0.0</td><td>0.0</td><td>0.090213</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌──────┬────────────────────────────────┬─────────────────────────────┬────────────────────────────┐\n",
       "│ name ┆ top-inview-articles_intralist_ ┆ popular_intralist_diversity ┆ random_intralist_diversity │\n",
       "│ ---  ┆ di…                            ┆ ---                         ┆ ---                        │\n",
       "│ str  ┆ ---                            ┆ f64                         ┆ f64                        │\n",
       "│      ┆ f64                            ┆                             ┆                            │\n",
       "╞══════╪════════════════════════════════╪═════════════════════════════╪════════════════════════════╡\n",
       "│ mean ┆ 0.790542                       ┆ 0.840236                    ┆ 0.754795                   │\n",
       "│ std  ┆ 0.0                            ┆ 0.0                         ┆ 0.090213                   │\n",
       "└──────┴────────────────────────────────┴─────────────────────────────┴────────────────────────────┘"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intralist_diversity_dict = {\"name\": [\"mean\", \"std\"]}\n",
    "\n",
    "for candidates, list_name in candidates_name_pairs:\n",
    "    scores = intralist_diversity(\n",
    "        candidates,\n",
    "        lookup_dict=articles_dict,\n",
    "        lookup_key=CONTRASTIVE_VECTOR,\n",
    "    )\n",
    "    intralist_diversity_dict[f\"{list_name}_{intralist_diversity.name}\"] = [\n",
    "        scores.mean(),\n",
    "        scores.std(),\n",
    "    ]\n",
    "\n",
    "write_json_file(\n",
    "    intralist_diversity_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_DIVERSITY),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(intralist_diversity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The embedding representation\n",
    "This might be obvious, but the embedding representation used for computing a metric is very influential. Hence, baselines are important to determine high and low scores. Also, this is why these metrics can be very hard to interpret for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrastive_vector: 0.7905415595169083\n",
      "document_vector: 0.1584846028291677\n"
     ]
    }
   ],
   "source": [
    "ROBERTA_EMB = \"FacebookAI/xlm-roberta-base\"\n",
    "BERT_EMB = \"google-bert/bert-base-multilingual-cased\"\n",
    "print(\n",
    "    f\"{CONTRASTIVE_VECTOR}: {intralist_diversity(top_n_candidates_editorial_picks, lookup_dict=articles_dict, lookup_key=CONTRASTIVE_VECTOR).mean()}\"\n",
    ")\n",
    "print(\n",
    "    f\"{DOCUMENT_VECTOR}: {intralist_diversity(top_n_candidates_editorial_picks, lookup_dict=articles_dict, lookup_key=DOCUMENT_VECTOR).mean()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/sentiment_score.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>top-inview-articles_sentiment</th><th>popular_sentiment</th><th>random_sentiment</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mean&quot;</td><td>0.7294</td><td>0.78342</td><td>0.821769</td></tr><tr><td>&quot;std&quot;</td><td>0.0</td><td>0.0</td><td>0.071807</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌──────┬───────────────────────────────┬───────────────────┬──────────────────┐\n",
       "│ name ┆ top-inview-articles_sentiment ┆ popular_sentiment ┆ random_sentiment │\n",
       "│ ---  ┆ ---                           ┆ ---               ┆ ---              │\n",
       "│ str  ┆ f64                           ┆ f64               ┆ f64              │\n",
       "╞══════╪═══════════════════════════════╪═══════════════════╪══════════════════╡\n",
       "│ mean ┆ 0.7294                        ┆ 0.78342           ┆ 0.821769         │\n",
       "│ std  ┆ 0.0                           ┆ 0.0               ┆ 0.071807         │\n",
       "└──────┴───────────────────────────────┴───────────────────┴──────────────────┘"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dict = {\"name\": [\"mean\", \"std\"]}\n",
    "\n",
    "for candidates, list_name in candidates_name_pairs:\n",
    "    scores = sentiment(\n",
    "        candidates,\n",
    "        lookup_dict=articles_dict,\n",
    "        lookup_key=DEFAULT_SENTIMENT_SCORE_COL,\n",
    "    )\n",
    "    sentiment_dict[f\"{list_name}_{sentiment.name}\"] = [\n",
    "        scores.mean(),\n",
    "        scores.std(),\n",
    "    ]\n",
    "\n",
    "write_json_file(\n",
    "    sentiment_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_SENTIMENT_SCORE),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(sentiment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/novelty.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>top-inview-articles_novelty</th><th>popular_novelty</th><th>random_novelty</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mean&quot;</td><td>4.625792</td><td>3.069874</td><td>8.36168</td></tr><tr><td>&quot;std&quot;</td><td>0.0</td><td>0.0</td><td>1.849578</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌──────┬─────────────────────────────┬─────────────────┬────────────────┐\n",
       "│ name ┆ top-inview-articles_novelty ┆ popular_novelty ┆ random_novelty │\n",
       "│ ---  ┆ ---                         ┆ ---             ┆ ---            │\n",
       "│ str  ┆ f64                         ┆ f64             ┆ f64            │\n",
       "╞══════╪═════════════════════════════╪═════════════════╪════════════════╡\n",
       "│ mean ┆ 4.625792                    ┆ 3.069874        ┆ 8.36168        │\n",
       "│ std  ┆ 0.0                         ┆ 0.0             ┆ 1.849578       │\n",
       "└──────┴─────────────────────────────┴─────────────────┴────────────────┘"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novelty_dict = {\"name\": [\"mean\", \"std\"]}\n",
    "\n",
    "for candidates, list_name in candidates_name_pairs:\n",
    "    scores = novelty(\n",
    "        candidates,\n",
    "        lookup_dict=articles_dict,\n",
    "        lookup_key=DEFAULT_TOTAL_PAGEVIEWS_COL_NORMALIZED_MIN_MAX,\n",
    "    )\n",
    "    novelty_dict[f\"{list_name}_{novelty.name}\"] = [\n",
    "        scores.mean(),\n",
    "        scores.std(),\n",
    "    ]\n",
    "\n",
    "write_json_file(\n",
    "    novelty_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_NOVELTY),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(novelty_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serendipity\n",
    "When computing Serendipity it using the user's history; similarity between recommendations and browsed items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/serendipity.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>top-inview-articles_serendipity</th><th>popular_serendipity</th><th>random_serendipity</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;mean&quot;</td><td>0.786146</td><td>0.791526</td><td>0.806839</td></tr><tr><td>&quot;std&quot;</td><td>0.026169</td><td>0.026284</td><td>0.03369</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌──────┬─────────────────────────────────┬─────────────────────┬────────────────────┐\n",
       "│ name ┆ top-inview-articles_serendipity ┆ popular_serendipity ┆ random_serendipity │\n",
       "│ ---  ┆ ---                             ┆ ---                 ┆ ---                │\n",
       "│ str  ┆ f64                             ┆ f64                 ┆ f64                │\n",
       "╞══════╪═════════════════════════════════╪═════════════════════╪════════════════════╡\n",
       "│ mean ┆ 0.786146                        ┆ 0.791526            ┆ 0.806839           │\n",
       "│ std  ┆ 0.026169                        ┆ 0.026284            ┆ 0.03369            │\n",
       "└──────┴─────────────────────────────────┴─────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serendipity_dict = {\"name\": [\"mean\", \"std\"]}\n",
    "for candidates, list_name in candidates_name_pairs:\n",
    "    if len(candidates) == 1:\n",
    "        candidates = np.tile(candidates, len(user_history)).reshape(-1, TOP_N)\n",
    "    #\n",
    "    scores = serendipity(\n",
    "        candidates,\n",
    "        H=user_history,\n",
    "        lookup_dict=articles_dict,\n",
    "        lookup_key=CONTRASTIVE_VECTOR,\n",
    "    )\n",
    "    serendipity_dict[f\"{list_name}_{serendipity.name}\"] = [scores.mean(), scores.std()]\n",
    "\n",
    "write_json_file(\n",
    "    serendipity_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_SERENDIPITY),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(serendipity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/coverage.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>top-inview-articles_coverage</th><th>popular_coverage</th><th>random_coverage</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>5.0</td><td>5.0</td><td>250.0</td></tr><tr><td>&quot;fraction&quot;</td><td>0.02</td><td>0.02</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌──────────┬──────────────────────────────┬──────────────────┬─────────────────┐\n",
       "│ name     ┆ top-inview-articles_coverage ┆ popular_coverage ┆ random_coverage │\n",
       "│ ---      ┆ ---                          ┆ ---              ┆ ---             │\n",
       "│ str      ┆ f64                          ┆ f64              ┆ f64             │\n",
       "╞══════════╪══════════════════════════════╪══════════════════╪═════════════════╡\n",
       "│ count    ┆ 5.0                          ┆ 5.0              ┆ 250.0           │\n",
       "│ fraction ┆ 0.02                         ┆ 0.02             ┆ 1.0             │\n",
       "└──────────┴──────────────────────────────┴──────────────────┴─────────────────┘"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_dict = {\"name\": [\"count\", \"fraction\"]}\n",
    "for candidates, list_name in candidates_name_pairs:\n",
    "    coverage_dict[f\"{list_name}_{coverage.name}\"] = coverage(candidates, candidate_list)\n",
    "\n",
    "write_json_file(\n",
    "    coverage_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_COVERAGE),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(coverage_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution - Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_distribution_dict(dict_: dict) -> dict:\n",
    "    output_results = (\n",
    "        pl.concat(\n",
    "            [pl.DataFrame(val) for val in dict_.values()],\n",
    "            how=\"diagonal\",\n",
    "        )\n",
    "        .with_row_index(name=\"name\")\n",
    "        .with_columns(pl.Series(dict_.keys()).alias(\"name\"))\n",
    "    ).to_dict()\n",
    "    return {key: val.to_list() for key, val in output_results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/distribution_category.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>forbrug</th><th>nyheder</th><th>krimi</th><th>sport</th><th>penge</th><th>auto</th><th>underholdning</th><th>musik</th><th>nationen</th><th>incoming</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;top-inview-art…</td><td>0.2</td><td>0.2</td><td>0.2</td><td>0.2</td><td>0.2</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;popular_novelt…</td><td>null</td><td>0.2</td><td>0.2</td><td>0.4</td><td>0.2</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;random_novelty…</td><td>0.011886</td><td>0.160379</td><td>0.095914</td><td>0.16759</td><td>0.036201</td><td>0.37992</td><td>0.084096</td><td>0.035976</td><td>0.023992</td><td>0.004046</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 11)\n",
       "┌──────────────┬──────────┬──────────┬──────────┬───┬─────────────┬──────────┬──────────┬──────────┐\n",
       "│ name         ┆ forbrug  ┆ nyheder  ┆ krimi    ┆ … ┆ underholdni ┆ musik    ┆ nationen ┆ incoming │\n",
       "│ ---          ┆ ---      ┆ ---      ┆ ---      ┆   ┆ ng          ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ str          ┆ f64      ┆ f64      ┆ f64      ┆   ┆ ---         ┆ f64      ┆ f64      ┆ f64      │\n",
       "│              ┆          ┆          ┆          ┆   ┆ f64         ┆          ┆          ┆          │\n",
       "╞══════════════╪══════════╪══════════╪══════════╪═══╪═════════════╪══════════╪══════════╪══════════╡\n",
       "│ top-inview-a ┆ 0.2      ┆ 0.2      ┆ 0.2      ┆ … ┆ null        ┆ null     ┆ null     ┆ null     │\n",
       "│ rticles_nove ┆          ┆          ┆          ┆   ┆             ┆          ┆          ┆          │\n",
       "│ lty          ┆          ┆          ┆          ┆   ┆             ┆          ┆          ┆          │\n",
       "│ popular_nove ┆ null     ┆ 0.2      ┆ 0.2      ┆ … ┆ null        ┆ null     ┆ null     ┆ null     │\n",
       "│ lty          ┆          ┆          ┆          ┆   ┆             ┆          ┆          ┆          │\n",
       "│ random_novel ┆ 0.011886 ┆ 0.160379 ┆ 0.095914 ┆ … ┆ 0.084096    ┆ 0.035976 ┆ 0.023992 ┆ 0.004046 │\n",
       "│ ty           ┆          ┆          ┆          ┆   ┆             ┆          ┆          ┆          │\n",
       "└──────────────┴──────────┴──────────┴──────────┴───┴─────────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMN = DEFAULT_CATEGORY_STR_COL\n",
    "distribution_dict = {\n",
    "    f\"{list_name}_{novelty.name}\": distribution(\n",
    "        candidates,\n",
    "        lookup_dict=articles_dict,\n",
    "        lookup_key=COLUMN,\n",
    "    )\n",
    "    for candidates, list_name in candidates_name_pairs\n",
    "}\n",
    "# =>\n",
    "distribution_category_dict = concat_distribution_dict(distribution_dict)\n",
    "\n",
    "write_json_file(\n",
    "    distribution_category_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_DISTRIBUTION_CATEGORY),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(distribution_category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution - Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/distribution_sentiment_label.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>Negative</th><th>Positive</th><th>Neutral</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;top-inview-art…</td><td>0.4</td><td>0.6</td><td>null</td></tr><tr><td>&quot;popular_novelt…</td><td>0.2</td><td>0.4</td><td>0.4</td></tr><tr><td>&quot;random_novelty…</td><td>0.396154</td><td>0.312102</td><td>0.291744</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌─────────────────────────────┬──────────┬──────────┬──────────┐\n",
       "│ name                        ┆ Negative ┆ Positive ┆ Neutral  │\n",
       "│ ---                         ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ str                         ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞═════════════════════════════╪══════════╪══════════╪══════════╡\n",
       "│ top-inview-articles_novelty ┆ 0.4      ┆ 0.6      ┆ null     │\n",
       "│ popular_novelty             ┆ 0.2      ┆ 0.4      ┆ 0.4      │\n",
       "│ random_novelty              ┆ 0.396154 ┆ 0.312102 ┆ 0.291744 │\n",
       "└─────────────────────────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMN = DEFAULT_SENTIMENT_LABEL_COL\n",
    "distribution_dict = {\n",
    "    f\"{list_name}_{novelty.name}\": distribution(\n",
    "        candidates,\n",
    "        lookup_dict=articles_dict,\n",
    "        lookup_key=COLUMN,\n",
    "    )\n",
    "    for candidates, list_name in candidates_name_pairs\n",
    "}\n",
    "# =>\n",
    "distribution_sentiment_dict = concat_distribution_dict(distribution_dict)\n",
    "\n",
    "write_json_file(\n",
    "    distribution_sentiment_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_DISTRIBUTION_SENTIMENT_LABEL),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(distribution_sentiment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution - Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing JSON: 'ebnerd_predictions/baselines/distribution_topics.json'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 61)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>Livsstil</th><th>Sundhed</th><th>Sygdom og behandling</th><th>Kendt</th><th>Bolig</th><th>Køb og salg</th><th>Kriminalitet</th><th>Bandekriminalitet</th><th>Sport</th><th>Erhverv</th><th>Privat virksomhed</th><th>Økonomi</th><th>Mikro</th><th>Politik</th><th>International politik</th><th>Personfarlig kriminalitet</th><th>Offentlig instans</th><th>Begivenhed</th><th>Fodbold</th><th>Sportsbegivenhed</th><th>Ketcher- og batsport</th><th>Ansættelsesforhold</th><th>Underholdning</th><th>National politik</th><th>Film og tv</th><th>Makro</th><th>Vejr</th><th>Katastrofe</th><th>Dyr</th><th>Musik og lyd</th><th>Underholdningsbegivenhed</th><th>Motorsport</th><th>Familieliv</th><th>Mindre ulykke</th><th>Større katastrofe</th><th>Transportmiddel</th><th>Større transportmiddel</th><th>Offentlig transport</th><th>Teknologi</th><th>Kunstig intelligens og software</th><th>Forbrugerelektronik</th><th>Bil</th><th>Mad og drikke</th><th>Samfund</th><th>Værdier</th><th>Konflikt og krig</th><th>Bæredygtighed og klima</th><th>Bedrageri</th><th>Væbnet konflikt</th><th>Reality</th><th>Partnerskab</th><th>Kosmetisk behandling</th><th>Kultur</th><th>Håndbold</th><th>Videnskab</th><th>Naturvidenskab</th><th>Uddannelse</th><th>Ungdomsuddannelse</th><th>Renovering og indretning</th><th>Udlejning</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;top-inview-art…</td><td>0.133333</td><td>0.066667</td><td>0.066667</td><td>0.133333</td><td>0.066667</td><td>0.066667</td><td>0.066667</td><td>0.066667</td><td>0.066667</td><td>0.066667</td><td>0.066667</td><td>0.066667</td><td>0.066667</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;popular_novelt…</td><td>null</td><td>null</td><td>null</td><td>0.157895</td><td>null</td><td>null</td><td>0.052632</td><td>null</td><td>0.105263</td><td>0.105263</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>0.052632</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;random_novelty…</td><td>0.016972</td><td>0.008972</td><td>0.007861</td><td>0.07182</td><td>0.069587</td><td>0.06503</td><td>0.038267</td><td>0.003397</td><td>0.045939</td><td>0.095745</td><td>0.07094</td><td>0.104557</td><td>0.00675</td><td>0.031369</td><td>0.013429</td><td>0.024699</td><td>0.002267</td><td>0.031291</td><td>0.030231</td><td>0.017845</td><td>0.00678</td><td>0.021373</td><td>0.029226</td><td>0.016832</td><td>0.007851</td><td>0.007883</td><td>0.002222</td><td>0.015724</td><td>0.00112</td><td>0.011237</td><td>0.011198</td><td>0.002232</td><td>0.002253</td><td>0.011225</td><td>0.001136</td><td>0.016908</td><td>0.005648</td><td>0.001156</td><td>0.004462</td><td>0.002265</td><td>0.001123</td><td>0.00675</td><td>0.004573</td><td>0.012429</td><td>0.00221</td><td>0.006748</td><td>0.002306</td><td>0.002265</td><td>0.002253</td><td>0.001114</td><td>0.002229</td><td>0.001145</td><td>0.002285</td><td>0.002227</td><td>0.001138</td><td>0.001138</td><td>0.00448</td><td>0.00448</td><td>0.001153</td><td>0.002251</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 61)\n",
       "┌────────────┬──────────┬──────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ name       ┆ Livsstil ┆ Sundhed  ┆ Sygdom og ┆ … ┆ Uddannels ┆ Ungdomsud ┆ Renoverin ┆ Udlejning │\n",
       "│ ---        ┆ ---      ┆ ---      ┆ behandlin ┆   ┆ e         ┆ dannelse  ┆ g og indr ┆ ---       │\n",
       "│ str        ┆ f64      ┆ f64      ┆ g         ┆   ┆ ---       ┆ ---       ┆ etning    ┆ f64       │\n",
       "│            ┆          ┆          ┆ ---       ┆   ┆ f64       ┆ f64       ┆ ---       ┆           │\n",
       "│            ┆          ┆          ┆ f64       ┆   ┆           ┆           ┆ f64       ┆           │\n",
       "╞════════════╪══════════╪══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ top-inview ┆ 0.133333 ┆ 0.066667 ┆ 0.066667  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ -articles_ ┆          ┆          ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ novelty    ┆          ┆          ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ popular_no ┆ null     ┆ null     ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ velty      ┆          ┆          ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ random_nov ┆ 0.016972 ┆ 0.008972 ┆ 0.007861  ┆ … ┆ 0.00448   ┆ 0.00448   ┆ 0.001153  ┆ 0.002251  │\n",
       "│ elty       ┆          ┆          ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴──────────┴──────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMN = DEFAULT_TOPICS_COL\n",
    "distribution_dict = {\n",
    "    f\"{list_name}_{novelty.name}\": distribution(\n",
    "        candidates,\n",
    "        lookup_dict=articles_dict,\n",
    "        lookup_key=COLUMN,\n",
    "    )\n",
    "    for candidates, list_name in candidates_name_pairs\n",
    "}\n",
    "# =>\n",
    "\n",
    "distribution_topics_dict = concat_distribution_dict(distribution_dict)\n",
    "\n",
    "write_json_file(\n",
    "    distribution_topics_dict,\n",
    "    PATH_BEYOND_ACCURACY_BASELINES.joinpath(BASELINE_DISTRIBUTION_TOPICS),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pl.DataFrame(distribution_topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Submission file\n",
    "In this example, we randomly rank the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>impression_id</th><th>article_ids_inview</th></tr><tr><td>u32</td><td>list[i64]</td></tr></thead><tbody><tr><td>6451339</td><td>[8, 1, … 3]</td></tr><tr><td>6451363</td><td>[5, 4, … 2]</td></tr><tr><td>6451382</td><td>[1, 5, … 2]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "┌───────────────┬────────────────────┐\n",
       "│ impression_id ┆ article_ids_inview │\n",
       "│ ---           ┆ ---                │\n",
       "│ u32           ┆ list[i64]          │\n",
       "╞═══════════════╪════════════════════╡\n",
       "│ 6451339       ┆ [8, 1, … 3]        │\n",
       "│ 6451363       ┆ [5, 4, … 2]        │\n",
       "│ 6451382       ┆ [1, 5, … 2]        │\n",
       "└───────────────┴────────────────────┘"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "df = (\n",
    "    pl.concat([df_behaviors, df_beyond_accuarcy])\n",
    "    .select(DEFAULT_IMPRESSION_ID_COL, DEFAULT_INVIEW_ARTICLES_COL)\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL).map_elements(\n",
    "            lambda x: list(np.random.permutation(len(x)) + 1)\n",
    "        )\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: how to convert prediction scores to argsorted output format\n",
    "A quick detour, to see how, you could convert actual prediction-scores to the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>impression_id</th><th>article_ids_inview</th></tr><tr><td>u32</td><td>list[f64]</td></tr></thead><tbody><tr><td>6451339</td><td>[0.125, 1.0, … 0.333333]</td></tr><tr><td>6451363</td><td>[0.2, 0.25, … 0.5]</td></tr><tr><td>6451382</td><td>[1.0, 0.2, … 0.5]</td></tr><tr><td>6451383</td><td>[0.142857, 0.166667, … 0.2]</td></tr><tr><td>6451385</td><td>[0.2, 0.333333, … 1.0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────────┬─────────────────────────────┐\n",
       "│ impression_id ┆ article_ids_inview          │\n",
       "│ ---           ┆ ---                         │\n",
       "│ u32           ┆ list[f64]                   │\n",
       "╞═══════════════╪═════════════════════════════╡\n",
       "│ 6451339       ┆ [0.125, 1.0, … 0.333333]    │\n",
       "│ 6451363       ┆ [0.2, 0.25, … 0.5]          │\n",
       "│ 6451382       ┆ [1.0, 0.2, … 0.5]           │\n",
       "│ 6451383       ┆ [0.142857, 0.166667, … 0.2] │\n",
       "│ 6451385       ┆ [0.2, 0.333333, … 1.0]      │\n",
       "└───────────────┴─────────────────────────────┘"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_score = df[:100].with_columns(\n",
    "    pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.eval(1 / pl.element())\n",
    ")\n",
    "rank_score.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>impression_id</th><th>article_ids_inview</th><th>prediction_scores</th></tr><tr><td>u32</td><td>list[f64]</td><td>list[i64]</td></tr></thead><tbody><tr><td>6451339</td><td>[0.125, 1.0, … 0.333333]</td><td>[8, 1, … 3]</td></tr><tr><td>6451363</td><td>[0.2, 0.25, … 0.5]</td><td>[5, 4, … 2]</td></tr><tr><td>6451382</td><td>[1.0, 0.2, … 0.5]</td><td>[1, 5, … 2]</td></tr><tr><td>6451383</td><td>[0.142857, 0.166667, … 0.2]</td><td>[7, 6, … 5]</td></tr><tr><td>6451385</td><td>[0.2, 0.333333, … 1.0]</td><td>[5, 3, … 1]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌───────────────┬─────────────────────────────┬───────────────────┐\n",
       "│ impression_id ┆ article_ids_inview          ┆ prediction_scores │\n",
       "│ ---           ┆ ---                         ┆ ---               │\n",
       "│ u32           ┆ list[f64]                   ┆ list[i64]         │\n",
       "╞═══════════════╪═════════════════════════════╪═══════════════════╡\n",
       "│ 6451339       ┆ [0.125, 1.0, … 0.333333]    ┆ [8, 1, … 3]       │\n",
       "│ 6451363       ┆ [0.2, 0.25, … 0.5]          ┆ [5, 4, … 2]       │\n",
       "│ 6451382       ┆ [1.0, 0.2, … 0.5]           ┆ [1, 5, … 2]       │\n",
       "│ 6451383       ┆ [0.142857, 0.166667, … 0.2] ┆ [7, 6, … 5]       │\n",
       "│ 6451385       ┆ [0.2, 0.333333, … 1.0]      ┆ [5, 3, … 1]       │\n",
       "└───────────────┴─────────────────────────────┴───────────────────┘"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_score.with_columns(\n",
    "    pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"prediction_scores\")\n",
    ").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13536710it [00:19, 712081.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping ebnerd_predictions/predictions.txt to ebnerd_predictions/predictions_ebnerd_large_random.zip\n"
     ]
    }
   ],
   "source": [
    "impression_ids = df[DEFAULT_IMPRESSION_ID_COL].to_list()\n",
    "prediction_scores = df[DEFAULT_INVIEW_ARTICLES_COL].to_list()\n",
    "\n",
    "write_submission_file(\n",
    "    impression_ids=impression_ids,\n",
    "    prediction_scores=prediction_scores,\n",
    "    path=PATH_BEYOND_ACCURACY.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"predictions_{DATASPLIT}_random.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
